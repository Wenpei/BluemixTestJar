sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
#twostepmodel <- clustering.twostep(df,Species~.)
twostepmodel2 <- clustering.twostep(df, formula=NULL, inputFieldList=c("Sepal_Length", "Sepal_Width", "Petal_Length", "Petal_Width"))
scoredata <- spss.predict(twostepmodel,df)
head(scoredata)
scoredata <- spss.predict(twostepmodel2,df)
head(scoredata)
library(SPSSonSparkR)
spss.summary()
spss.summary()
spss.summary()
library(SparkR)
Sys.setenv(SPARK_HOME = "C:\\aWorkFolder\\spark\\spark-1.6.0-bin-hadoop2.6\\spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
#twostepmodel <- clustering.twostep(df,Species~.)
twostepmodel2 <- clustering.twostep(df, formula=NULL, inputFieldList=c("Sepal_Length", "Sepal_Width", "Petal_Length", "Petal_Width"))
scoredata <- spss.predict(twostepmodel2,df)
head(scoredata)
a <- TwoStepModel()
SPSSModel()
SPSSModel(sc)
SPSSModel(jobj=sc)
library(SPSSonSparkR)
library(SparkR)
Sys.setenv(SPARK_HOME = "C:\\aWorkFolder\\spark\\spark-1.6.0-bin-hadoop2.6\\spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
#twostepmodel <- clustering.twostep(df,Species~.)
twostepmodel2 <- clustering.twostep(df, formula=NULL, inputFieldList=c("Sepal_Length", "Sepal_Width", "Petal_Length", "Petal_Width"))
scoredata <- spss.predict(twostepmodel2,df)
head(scoredata)
inputFieldList <- c("Sepal_Length", "Sepal_Width", "Petal_Length", "Petal_Width")
jobj = spss.TwoStep.fit(data, NULL, inputFieldList)
jobj
jobj
new("TwoStepModel", "aaa", jobj)
new("TwoStepModel", name = "aaa", jobj = jobj)
new("TwoStepModel", name = "aaa", new("SPSSModel", jobj=jobj))
tsmodel <- new("TwoStepModel", name = "aaa", new("SPSSModel", jobj=jobj))
tsmodel
scoredata <- spss.predict(tsmodel,df)
head(scoredata)
SPSSModel(jobj)
SPSSModel(jobj=jobj)
SPSSModel(jobj=jobj)
SPSSModel <-
setClass("SPSSModel", slots = list(jobj = "jobj"))
SPSSModel(jobj=jobj)
library(SPSSonSparkR)
library(SparkR)
Sys.setenv(SPARK_HOME = "C:\\aWorkFolder\\spark\\spark-1.6.0-bin-hadoop2.6\\spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
#twostepmodel <- clustering.twostep(df,Species~.)
twostepmodel2 <- clustering.twostep(df, formula=NULL, inputFieldList=c("Sepal_Length", "Sepal_Width", "Petal_Length", "Petal_Width"))
scoredata <- spss.predict(twostepmodel2,df)
head(scoredata)
library(SPSSonSparkR)
library(SparkR)
Sys.setenv(SPARK_HOME = "C:\\aWorkFolder\\spark\\spark-1.6.0-bin-hadoop2.6\\spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
#twostepmodel <- clustering.twostep(df,Species~.)
twostepmodel2 <- clustering.twostep(df, formula=NULL, inputFieldList=c("Sepal_Length", "Sepal_Width", "Petal_Length", "Petal_Width"))
scoredata <- spss.predict(twostepmodel2,df)
head(scoredata)
library(SPSSonSparkR)
library(SparkR)
Sys.setenv(SPARK_HOME = "C:\\aWorkFolder\\spark\\spark-1.6.0-bin-hadoop2.6\\spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
twostepmodel <- clustering.twostep(df,Species~.)
#twostepmodel2 <- clustering.twostep(df, formula=NULL, inputFieldList=c("Sepal_Length", "Sepal_Width", "Petal_Length", "Petal_Width"))
scoredata <- spss.predict(twostepmodel,df)
head(scoredata)
sparkR.stop()
library(SparkR)
Sys.setenv(SPARK_HOME = "C:\\aWorkFolder\\spark\\spark-1.6.0-bin-hadoop2.6\\spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
twostepmodel <- clustering.twostep(df,Species~.)
#twostepmodel2 <- clustering.twostep(df, formula=NULL, inputFieldList=c("Sepal_Length", "Sepal_Width", "Petal_Length", "Petal_Width"))
scoredata <- spss.predict(twostepmodel,df)
head(scoredata)
c()
c()
c("")
is.null(c())
library(SPSSonSparkR)
head(scoredata)
library(SparkR)
Sys.setenv(SPARK_HOME = "C:\\aWorkFolder\\spark\\spark-1.6.0-bin-hadoop2.6\\spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
twostepmodel <- clustering.twostep(df,Species~.)
#twostepmodel2 <- clustering.twostep(df, formula=NULL, inputFieldList=c("Sepal_Length", "Sepal_Width", "Petal_Length", "Petal_Width"))
scoredata <- spss.predict(twostepmodel,df)
head(scoredata)
spss.TwoStep.fit <- function(
data,
formula,
inputFieldList,
standardizeFieldList,
#autoClustering,
#featureSelection,
#outlierHandling,
...
){
twostep <- SparkR:::callJStatic("com.ibm.spss.ml.clustering.TwoStep","apply")
#SparkR:::callJMethod(twostep,"setAutoClustering",autoClustering)
#SparkR:::callJMethod(twostep,"setFeatureSelection",featureSelection)
#SparkR:::callJMethod(twostep,"setOutlierHandling",outlierHandling)
advanceArgs = list(...)
print(advanceArgs)
lapply(seq_along(advanceArgs), function(i){
name <- names(advanceArgs[i])
head <- toupper(substr(name,0,1))
tril <- substring(name,2)
name <- paste("set",head,tril,sep="")
print(name)
value <- spss.util.parameterSwitch(spss.twostep.parameterType, name, advanceArgs[[i]])
print(value)
SparkR:::callJMethod(twostep,name,value)
})
if(!is.null(formula)){
formula <- paste(deparse(formula), collapse = "")
rmodel <- SparkR:::callJStatic("com.ibm.spss.ml.r.RWrapper","fitWithRFormula", twostep, formula, df@sdf)
}else{
SparkR:::callJMethod(twostep,"setInputFieldList",as.array(inputFieldList))
if(!is.null(standardizeFieldList))
SparkR:::callJMethod(twostep,"setStandardizeFieldList",as.array(standardizeFieldList))
rmodel <- SparkR:::callJStatic("com.ibm.spss.ml.r.RWrapper","fit", twostep, df@sdf)
}
rmodel
}
twostepmodel <- clustering.twostep(df,Species~.)
spss.TwoStep.fit(df, Species~.)
spss.TwoStep.fit(df, Species~., c(), c(), TRUE, TRUE, TRUE)
spss.TwoStep.fit(df, Species~., c(), c(), TRUE)
spss.TwoStep.fit(df, Species~., c(), c(), featureSelection = TRUE)
spss.TwoStep.fit <- function(
data,
formula,
inputFieldList,
standardizeFieldList,
#autoClustering,
#featureSelection,
#outlierHandling,
...
){
twostep <- SparkR:::callJStatic("com.ibm.spss.ml.clustering.TwoStep","apply")
#SparkR:::callJMethod(twostep,"setAutoClustering",autoClustering)
#SparkR:::callJMethod(twostep,"setFeatureSelection",featureSelection)
#SparkR:::callJMethod(twostep,"setOutlierHandling",outlierHandling)
advanceArgs = list(...)
print(advanceArgs)
lapply(seq_along(advanceArgs), function(i){
parameterName <- names(advanceArgs[i])
head <- toupper(substr(name,0,1))
tril <- substring(name,2)
functionName <- paste("set",head,tril,sep="")
print(name)
value <- spss.util.parameterSwitch(spss.twostep.parameterType, name, advanceArgs[[i]])
print(value)
SparkR:::callJMethod(twostep,functionName,advanceArgs[[i]])
})
if(!is.null(formula)){
formula <- paste(deparse(formula), collapse = "")
rmodel <- SparkR:::callJStatic("com.ibm.spss.ml.r.RWrapper","fitWithRFormula", twostep, formula, df@sdf)
}else{
SparkR:::callJMethod(twostep,"setInputFieldList",as.array(inputFieldList))
if(!is.null(standardizeFieldList))
SparkR:::callJMethod(twostep,"setStandardizeFieldList",as.array(standardizeFieldList))
rmodel <- SparkR:::callJStatic("com.ibm.spss.ml.r.RWrapper","fit", twostep, df@sdf)
}
rmodel
}
spss.TwoStep.fit(df, Species~., featureSelection = TRUE)
# Major function call for twostep
spss.TwoStep.fit <- function(
data,
formula,
inputFieldList,
standardizeFieldList,
#autoClustering,
#featureSelection,
#outlierHandling,
...
){
twostep <- SparkR:::callJStatic("com.ibm.spss.ml.clustering.TwoStep","apply")
#SparkR:::callJMethod(twostep,"setAutoClustering",autoClustering)
#SparkR:::callJMethod(twostep,"setFeatureSelection",featureSelection)
#SparkR:::callJMethod(twostep,"setOutlierHandling",outlierHandling)
advanceArgs = list(...)
print(advanceArgs)
lapply(seq_along(advanceArgs), function(i){
parameterName <- names(advanceArgs[i])
head <- toupper(substr(parameterName,0,1))
tril <- substring(parameterName,2)
functionName <- paste("set",head,tril,sep="")
print(functionName)
value <- spss.util.parameterSwitch(spss.twostep.parameterType, parameterName, advanceArgs[[i]])
print(value)
SparkR:::callJMethod(twostep,functionName,advanceArgs[[i]])
})
if(!is.null(formula)){
formula <- paste(deparse(formula), collapse = "")
rmodel <- SparkR:::callJStatic("com.ibm.spss.ml.r.RWrapper","fitWithRFormula", twostep, formula, df@sdf)
}else{
SparkR:::callJMethod(twostep,"setInputFieldList",as.array(inputFieldList))
if(!is.null(standardizeFieldList))
SparkR:::callJMethod(twostep,"setStandardizeFieldList",as.array(standardizeFieldList))
rmodel <- SparkR:::callJStatic("com.ibm.spss.ml.r.RWrapper","fit", twostep, df@sdf)
}
rmodel
}
spss.TwoStep.fit(df, Species~., featureSelection = TRUE)
spss.util.parameterSwitch(spss.twostep.parameterType, "featureSelection", TRUE)
spss.TwoStep.fit <- function(
data,
formula,
inputFieldList,
standardizeFieldList,
#autoClustering,
#featureSelection,
#outlierHandling,
...
){
twostep <- SparkR:::callJStatic("com.ibm.spss.ml.clustering.TwoStep","apply")
#SparkR:::callJMethod(twostep,"setAutoClustering",autoClustering)
#SparkR:::callJMethod(twostep,"setFeatureSelection",featureSelection)
#SparkR:::callJMethod(twostep,"setOutlierHandling",outlierHandling)
advanceArgs = list(...)
print(advanceArgs)
lapply(seq_along(advanceArgs), function(i){
parameterName <- names(advanceArgs[i])
head <- toupper(substr(parameterName,0,1))
tril <- substring(parameterName,2)
functionName <- paste("set",head,tril,sep="")
print(functionName)
value <- spss.util.parameterSwitch(spss.TwoStep.parameterType, parameterName, advanceArgs[[i]])
print(value)
SparkR:::callJMethod(twostep,functionName,value)
})
if(!is.null(formula)){
formula <- paste(deparse(formula), collapse = "")
rmodel <- SparkR:::callJStatic("com.ibm.spss.ml.r.RWrapper","fitWithRFormula", twostep, formula, df@sdf)
}else{
SparkR:::callJMethod(twostep,"setInputFieldList",as.array(inputFieldList))
if(!is.null(standardizeFieldList))
SparkR:::callJMethod(twostep,"setStandardizeFieldList",as.array(standardizeFieldList))
rmodel <- SparkR:::callJStatic("com.ibm.spss.ml.r.RWrapper","fit", twostep, df@sdf)
}
rmodel
}
spss.TwoStep.fit(df, Species~., featureSelection = TRUE)
# Major function call for twostep
spss.TwoStep.fit <- function(
data,
formula,
inputFieldList,
standardizeFieldList,
#autoClustering,
#featureSelection,
#outlierHandling,
...
){
twostep <- SparkR:::callJStatic("com.ibm.spss.ml.clustering.TwoStep","apply")
#SparkR:::callJMethod(twostep,"setAutoClustering",autoClustering)
#SparkR:::callJMethod(twostep,"setFeatureSelection",featureSelection)
#SparkR:::callJMethod(twostep,"setOutlierHandling",outlierHandling)
advanceArgs = list(...)
lapply(seq_along(advanceArgs), function(i){
parameterName <- names(advanceArgs[i])
head <- toupper(substr(parameterName,0,1))
tril <- substring(parameterName,2)
functionName <- paste("set",head,tril,sep="")
#print(functionName)
value <- spss.util.parameterSwitch(spss.TwoStep.parameterType, parameterName, advanceArgs[[i]])
print(paste(functionName, "(", value, ")"))
SparkR:::callJMethod(twostep,functionName,value)
})
if(!is.null(formula)){
formula <- paste(deparse(formula), collapse = "")
rmodel <- SparkR:::callJStatic("com.ibm.spss.ml.r.RWrapper","fitWithRFormula", twostep, formula, df@sdf)
}else{
SparkR:::callJMethod(twostep,"setInputFieldList",as.array(inputFieldList))
if(!is.null(standardizeFieldList))
SparkR:::callJMethod(twostep,"setStandardizeFieldList",as.array(standardizeFieldList))
rmodel <- SparkR:::callJStatic("com.ibm.spss.ml.r.RWrapper","fit", twostep, df@sdf)
}
rmodel
}
spss.TwoStep.fit(df, Species~., featureSelection = TRUE)
library(SPSSonSparkR)
library(SparkR)
Sys.setenv(SPARK_HOME = "C:\\aWorkFolder\\spark\\spark-1.6.0-bin-hadoop2.6\\spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
twostepmodel <- clustering.twostep(df,Species~.)
#twostepmodel2 <- clustering.twostep(df, formula=NULL, inputFieldList=c("Sepal_Length", "Sepal_Width", "Petal_Length", "Petal_Width"))
scoredata <- spss.predict(twostepmodel,df)
head(scoredata)
spss.save(twostepmodel,"C:\\aWorkFolder\\data\\rwrappermodel3.model", TRUE)
spss.write_internal <- function(object, path, overwrite = FALSE) {
writer <- SparkR:::callJMethod(object@jobj, "write")
if (overwrite) {
writer <- SparkR:::callJMethod(writer, "overwrite")
}
invisible(SparkR:::callJMethod(writer, "save", path))
}
spss.save(twostepmodel,"C:\\aWorkFolder\\data\\rwrappermodel3.model", TRUE)
setMethod("spss.save", signature(object = "SPSSModel"),
function(object, path, overwrite = FALSE) {
spss.write_internal(object, path, overwrite)
})
spss.save(twostepmodel,"C:\\aWorkFolder\\data\\rwrappermodel3.model", TRUE)
spss.load_internal <- function(path) {
path <- suppressWarnings(normalizePath(path))
jobj <- SparkR:::callJStatic("com.ibm.spss.ml.r.RWrapper", "load", path)
SPSSModel(jobj = jobj)
}
spss.load <- function(path) {
spss.load_internal(path)
}
loadmodel <- spss.load("C:\\aWorkFolder\\data\\rwrappermodel3.model")
loadmodel
twostepmodel
head(spss.predict(loadmodel,df))
library(SPSSonSparkR)
help("clustering.twostep")
library(SPSSonSparkR)
help("clustering.twostep")
f1 <- function(a=c("a","b")){print(a)}
f1
f1("a")
f1("cc")
f1 <- function(a=c("a","b")){print(match.arg(a))}
f1
f1(c)
f1("c")
f1(c("a","b"))
f1(c("a","b"))
f1()
library(SPSSonSparkR)
library(SparkR)
Sys.setenv(SPARK_HOME = "C:\\aWorkFolder\\spark\\spark-1.6.0-bin-hadoop2.6\\spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
linearmodel <- classificationandregression.linearregression(df,Species~.)
scoredata <- spss.predict(linearmodel,df)
head(scoredata)
setMethod("classificationandregression.linearregression", signature(data = "DataFrame"),
function(
data,
formula = NULL,
inputFieldList = NULL,
targetField = NULL,
freqField = NULL,
recordIDFieldList = NULL,
regrWeightField = NULL,
factorSortOrder = c("ASCENDING","DESCENDING"),
intercept = TRUE,
plotCutPointNum = 19,
turnOnFactorSortOrder = TRUE,
useCustomMaxEffects = FALSE,
varSelectionMethod = c("bestSubsets", "forwardStepwise", "lasso", "ridge", "elasticNet", "none"),
...){
factorSortOrder <- match.arg(factorSortOrder)
varSelectionMethod <- match.arg(varSelectionMethod)
LinearRegressionModel(
SPSSModel(jobj =
spss.LinearRegression.fit(data, formula, inputFieldList, targetField,
freqField = freqField,
recordIDFieldList = recordIDFieldList,
regrWeightField = regrWeightField,
factorSortOrder = factorSortOrder,
intercept = intercept,
plotCutPointNum = plotCutPointNum,
turnOnFactorSortOrder = turnOnFactorSortOrder,
useCustomMaxEffects = useCustomMaxEffects,
varSelectionMethod = varSelectionMethod,
...)),
name = "LinearRegressionModel")
})
linearmodel <- classificationandregression.linearregression(df,Species~.)
library(SPSSonSparkR)
head(scoredata)
sparkR.stop()
library(SparkR)
Sys.setenv(SPARK_HOME = "C:\\aWorkFolder\\spark\\spark-1.6.0-bin-hadoop2.6\\spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
linearmodel <- classificationandregression.linearregression(df,Species~.)
scoredata <- spss.predict(linearmodel,df)
head(scoredata)
library(SparkR)
Sys.setenv(SPARK_HOME = "C:\\aWorkFolder\\spark\\spark-1.6.0-bin-hadoop2.6\\spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
linearmodel <- classificationandregression.linearregression(df,Species~.)
scoredata <- spss.predict(linearmodel,df)
head(scoredata)
linearmodel <- classificationandregression.linearregression(df,Species~.)
linearmodel <- classificationandregression.linearregression(df,Species~.)
linearmodel <- classificationandregression.linearregression(df,Species~.)
linearmodel <- classificationandregression.linearregression(df,Species~.)
linearmodel <- classificationandregression.linearregression(df,Species~.)
library(SparkR)
Sys.setenv(SPARK_HOME = "C:\\aWorkFolder\\spark\\spark-1.6.0-bin-hadoop2.6\\spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
linearmodel <- classificationandregression.linearregression(df,Species~.)
scoredata <- spss.predict(linearmodel,df)
head(scoredata)
library(SPSSonSparkR)
twostepmodel <- clustering.twostep(df,Species~.)
library(SPSSonSparkR)
library(SparkR)
Sys.setenv(SPARK_HOME = "C:\\aWorkFolder\\spark\\spark-1.6.0-bin-hadoop2.6\\spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
linearmodel <- classificationandregression.linearregression(df,Species~.)
scoredata <- spss.predict(linearmodel,df)
head(scoredata)
twostepmodel <- clustering.twostep(df,Species~.)
spss.summary(twostepmodel)
linearmodel <- classificationandregression.linearregression(df,Species~.)
linear <- SparkR:::callJStatic("com.ibm.spss.ml.classificationandregression.LinearRegression","apply")
SparkR:::callJMethod(linear,setPlotCutPointNum,19)
SparkR:::callJMethod(linear,"setPlotCutPointNum",19)
SparkR:::callJMethod(linear,"setPlotCutPointNum",as.integer(19))
type
typeof(19)
typeof(as.integer(19))
value <- spss.util.parameterSwitch(spss.TwoStep.parameterType, "plotCutPointNum", 19)
value
typeof(value)
library(SPSSonSparkR)
library(SparkR)
Sys.setenv(SPARK_HOME = "C:\\aWorkFolder\\spark\\spark-1.6.0-bin-hadoop2.6\\spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
linearmodel <- classificationandregression.linearregression(df,Species~.)
scoredata <- spss.predict(linearmodel,df)
head(scoredata)
library(SparkR)
Sys.setenv(SPARK_HOME = "C:\\aWorkFolder\\spark\\spark-1.6.0-bin-hadoop2.6\\spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
linearmodel <- classificationandregression.linearregression(df,Species~.)
scoredata <- spss.predict(linearmodel,df)
head(scoredata)
library(SPSSonSparkR)
is.null(NULL)
library(SparkR)
Sys.setenv(SPARK_HOME = "C:\\aWorkFolder\\spark\\spark-1.6.0-bin-hadoop2.6\\spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
data(iris)
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, iris)
linearmodel <- classificationandregression.linearregression(df,Species~.)
scoredata <- spss.predict(linearmodel,df)
head(scoredata)
library(SPSSonSparkR)
linearmodel <- classificationandregression.linearregression(df,inputFieldList=c("Sepal_Length", "Sepal_Width", "Petal_Length"),targetField = "Petal_Width")
linearmodel
spss.summary(linearmodel)
spss.predict(linearmodel,df)
head(spss.predict(linearmodel,df))
linearmodel <- classificationandregression.linearregression(df,Species~.)
linearmodel <- classificationandregression.linearregression(df,Sepal_Length~Sepal_Width+Petal_Length+Petal_Width)
linearmodel
str(iris)
install.packages("sparklyr")
library(sparklyr)
sc <- spark_connect(master = "local")
spark_install(version = "1.6.0")
